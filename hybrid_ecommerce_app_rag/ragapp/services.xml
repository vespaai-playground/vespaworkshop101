<?xml version="1.0" encoding="utf-8" ?>

<services version="1.0" xmlns:deploy="vespa" xmlns:preprocess="properties" minimum-required-vespa-version="8.338.38">

    <!-- See https://docs.vespa.ai/en/reference/services-container.html -->
    <container id="default" version="1.0">

        <clients>
            <!-- 
              Required for mTLS to still work, the below
              configuration is equivalent to using a single
              security/clients.pem
            -->
            <client id="mtls" permissions="read,write">
              <certificate file="security/clients.pem"/>
            </client>
        </clients>

        <!-- HuggingFace embedder component for generating embeddings from text -->
        <!-- See https://docs.vespa.ai/en/embedding.html#huggingface-embedder -->
        <component id="e5" type="hugging-face-embedder">
          <transformer-model model-id="e5-small-v2"/>
        </component>

        <!-- TODO: Setup the client to OpenAI. NOTE: you only use OpenAI for RAG, not for embeddings
            At least in this exercise. -->
        <!-- OpenAI LLM Client for RAG -->
        <component id="openai" class="ai.vespa.llm.clients.OpenAI">
            <config name="ai.vespa.llm.clients.llm-client">
            <apiKeySecretName>openai_api_key</apiKeySecretName>
            <model>gpt-4o-mini</model>
            </config>
        </component>

        <!--
            <document-api> tells the container that it should accept documents for indexing. Through the
            Document REST API you can PUT new documents, UPDATE existing documents, and DELETE documents
            already in the cluster.

            Documents sent to the Document REST API will be passed through document processors on the way
            to the content cluster.

            See:
             - Reference: https://docs.vespa.ai/en/reference/services-container.html#document-api
             - Operations: https://docs.vespa.ai/en/document-v1-api-guide.html
        -->
        <document-api/>

        <!--
            <search> tells the container to answers queries and serve results for those queries.
            Inside the <search /> cluster you can configure chains of "searchers" -
            Java components processing the query and/or result.

            See:
             - Reference: https://docs.vespa.ai/en/query-api.html
             - Searchers: https://docs.vespa.ai/en/searcher-development.html
        -->
        <!-- TODO in here, define a search chain pointing to your OpenAI client (only for RAG, not for embeddings) -->
        <search>
            <chain id="default" inherits="vespa">
            <searcher id="ai.vespa.search.llm.LLMSearcher" />
            </chain>

            <chain id="rag" inherits="vespa">
            <searcher id="ai.vespa.search.llm.RAGSearcher">
                <config name="ai.vespa.search.llm.llm-searcher">
                <providerId>openai</providerId>
                <stream>true</stream>
                </config>
            </searcher>
            </chain>
        </search>

        <!--
            <nodes> specifies the nodes that should run this cluster.
        -->
        <nodes>
            <node hostalias="node1" />
        </nodes>
        
    </container>

    <!-- See https://docs.vespa.ai/en/reference/services-content.html -->
    <content id="content" version="1.0">
        <min-redundancy>2</min-redundancy>
        <documents>
            <document type="product" mode="index" />
        </documents>
        <nodes>
            <node hostalias="node1" distribution-key="0" />
        </nodes>
    </content>

</services>
